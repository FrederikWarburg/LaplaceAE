{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from src.data import get_data\n",
    "from src.models import get_encoder, get_decoder\n",
    "from src.utils import softclip\n",
    "from src.hessian import sampler\n",
    "import yaml\n",
    "from copy import deepcopy\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def get_model(encoder, decoder):\n",
    "    \n",
    "    net = deepcopy(encoder.encoder._modules)\n",
    "    decoder = decoder.decoder._modules\n",
    "    max_ = max([int(i) for i in net.keys()])\n",
    "    for i in decoder.keys():\n",
    "        net.update({f\"{max_+int(i) + 1}\": decoder[i]})\n",
    "\n",
    "    return nn.Sequential(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ae = \"../weights/mnist/ae_[use_var_dec=False]\"\n",
    "with open(f\"{path_ae}/config.yaml\") as file:\n",
    "    config = yaml.full_load(file)\n",
    "ae_encoder = get_encoder(config, latent_size=2)\n",
    "ae_encoder.load_state_dict(torch.load(f\"{path_ae}/encoder.pth\"))\n",
    "\n",
    "\n",
    "path_mcae = \"../weights/mnist/mcdropout_ae\"\n",
    "with open(f\"{path_mcae}/config.yaml\") as file:\n",
    "    config = yaml.full_load(file)\n",
    "mcae_encoder = get_encoder(config, latent_size=2, dropout=config[\"dropout_rate\"])\n",
    "mcae_encoder.load_state_dict(torch.load(f\"{path_mcae}/encoder.pth\"))\n",
    "\n",
    "path_vae = \"../weights/mnist/vae_[use_var_dec=False]\"\n",
    "with open(f\"{path_vae}/config.yaml\") as file:\n",
    "    config = yaml.full_load(file)\n",
    "vae_encoder_mu = get_encoder(config, latent_size=2)\n",
    "vae_encoder_mu.load_state_dict(torch.load(f\"{path_vae}/mu_encoder.pth\"))\n",
    "vae_encoder_var = get_encoder(config, latent_size=2)\n",
    "vae_encoder_var.load_state_dict(torch.load(f\"{path_vae}/var_encoder.pth\"))\n",
    "\n",
    "#path_lae = \"../weights/mnist/lae_elbo/[backend_layer]_[approximation_exact]_[no_conv_True]_[train_samples_10]_/\"\n",
    "#path_lae = \"../weights/mnist/lae_elbo/[backend_layer]_[approximation_exact]_[no_conv_True]_[train_samples_1]_\"\n",
    "path_lae=\"../weights/mnist/lae_model_selection_7_may/weights_mnist_model_selection/1[backend_layer]_[approximation_exact]_[no_conv_True]_[train_samples_1]_\"\n",
    "with open(f\"{path_lae}/config.yaml\") as file:\n",
    "    config = yaml.full_load(file)\n",
    "lae_encoder = get_encoder(config, latent_size=2)\n",
    "lae_decoder = get_decoder(config, latent_size=2)\n",
    "lae_net = get_model(lae_encoder, lae_decoder).eval()\n",
    "lae_net.load_state_dict(torch.load(f\"{path_lae}/net.pth\"))\n",
    "hessian_approx = (\n",
    "    sampler.BlockSampler()\n",
    "    if config[\"approximation\"] == \"block\"\n",
    "    else sampler.DiagSampler()\n",
    ")\n",
    "h = torch.load(f\"{path_lae}/hessian.pth\")\n",
    "sigma_q = hessian_approx.invert(h).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50])\n",
      "torch.Size([1000, 1, 28, 28])\n",
      "torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "_, val_dataloader = get_data(\"mnist\")\n",
    "images, labels = [ ], [ ]\n",
    "\n",
    "for batch in val_dataloader:\n",
    "    x, y = batch\n",
    "    images.append(x)\n",
    "    labels.append(y)\n",
    "images = torch.cat(images, 0)\n",
    "labels = torch.cat(labels, 0)\n",
    "\n",
    "n_select = 5 # select this many datapoints from each class\n",
    "\n",
    "selected_images = torch.zeros(10*n_select, 1, 28, 28)\n",
    "selected_labels = torch.zeros(10*n_select,)\n",
    "eval_set_images = torch.zeros(images.shape[0]-10*n_select, 1, 28, 28)\n",
    "eval_set_labels = torch.zeros(images.shape[0]-10*n_select, )\n",
    "\n",
    "count = 0\n",
    "for i in range(10):\n",
    "    idx = torch.where(labels == i)[0]\n",
    "    n = len(idx)\n",
    "    rand_idx = torch.randperm(n)\n",
    "\n",
    "    selected_images[i*n_select:(i+1)*n_select] = images[idx[rand_idx[:n_select]]]\n",
    "    selected_labels[i*n_select:(i+1)*n_select] = labels[idx[rand_idx[:n_select]]]\n",
    "    eval_set_images[count:count+len(rand_idx[n_select:])] = images[idx[rand_idx[n_select:]]]\n",
    "    eval_set_labels[count:count+len(rand_idx[n_select:])] = labels[idx[rand_idx[n_select:]]]\n",
    "\n",
    "    count += len(rand_idx[n_select:])\n",
    "\n",
    "# just for testing purpose\n",
    "eval_set_images = eval_set_images[:1000]\n",
    "eval_set_labels = eval_set_labels[:1000]\n",
    "\n",
    "print(selected_images.shape)\n",
    "print(selected_labels.shape)\n",
    "print(eval_set_images.shape)\n",
    "print(eval_set_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard ae\n",
      "tensor([[-8.9853, -2.1847]], grad_fn=<AddmmBackward0>)\n",
      "monte-carlo ae\n",
      "tensor([[-11.7726,   9.0926],\n",
      "        [-12.8760,   6.9316]], grad_fn=<CatBackward0>)\n",
      "vae\n",
      "tensor([[ 1.5315, -0.8165],\n",
      "        [-0.4371, -0.9594]], grad_fn=<CatBackward0>)\n",
      "lae\n",
      "tensor([[0.7303, 1.1691],\n",
      "        [0.9279, 1.1496]])\n"
     ]
    }
   ],
   "source": [
    "def get_encoding_ae(data, n=1):\n",
    "    return ae_encoder(data)\n",
    "\n",
    "print('standard ae')\n",
    "print(get_encoding_ae(images[:1]))\n",
    "\n",
    "def get_encoding_mcae(data, n=1):\n",
    "    return torch.cat([mcae_encoder(data) for _ in range(n)], 0)\n",
    "\n",
    "print('monte-carlo ae')\n",
    "print(get_encoding_mcae(images[:1], n=2))\n",
    "\n",
    "def get_encoding_vae(data, n=1):\n",
    "    mu = vae_encoder_mu(data)\n",
    "    sigma = torch.exp(softclip(vae_encoder_var(data), min=-3))\n",
    "    return torch.cat([mu + torch.randn_like(sigma) * sigma for _ in range(n)])\n",
    "\n",
    "print('vae')\n",
    "print(get_encoding_vae(images[:1], n=2))\n",
    "\n",
    "def get_encoding_lae(data, n=1, layer=5):\n",
    "    embeddings = [ ]\n",
    "    def fw_hook_get_latent(module, input, output):\n",
    "        embeddings.append(output.detach().cpu())\n",
    "    temp = deepcopy(lae_net)\n",
    "    temp[layer].register_forward_hook(fw_hook_get_latent)\n",
    "\n",
    "    mu_q = parameters_to_vector(temp.parameters())\n",
    "    sample = hessian_approx.sample(mu_q, sigma_q, n_samples=n)\n",
    "    for i in range(n):\n",
    "        vector_to_parameters(sample[i], temp.parameters())\n",
    "        _ = temp(data)\n",
    "    return torch.cat(embeddings, 0)\n",
    "\n",
    "print('lae')\n",
    "print(get_encoding_lae(images[:1], n=2))\n",
    "\n",
    "def gen_encoding_lae5(data, n=1):\n",
    "    return get_encoding_lae(data, n=n, layer=5)\n",
    "\n",
    "def gen_encoding_lae3(data, n=1):\n",
    "    return get_encoding_lae(data, n=n, layer=3)\n",
    "\n",
    "def gen_encoding_lae7(data, n=1):\n",
    "    return get_encoding_lae(data, n=n, layer=7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nsde\\Anaconda3\\envs\\laplace\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_encoding_ae' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nsde\\Documents\\LaplaceAE\\notebooks\\semi.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nsde/Documents/LaplaceAE/notebooks/semi.ipynb#ch0000004?line=6'>7</a>\u001b[0m res_mean, res_std \u001b[39m=\u001b[39m [], []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nsde/Documents/LaplaceAE/notebooks/semi.ipynb#ch0000004?line=7'>8</a>\u001b[0m reps \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nsde/Documents/LaplaceAE/notebooks/semi.ipynb#ch0000004?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (name, encoder) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(names, [get_encoding_ae, get_encoding_mcae, get_encoding_vae, get_encoding_lae])):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nsde/Documents/LaplaceAE/notebooks/semi.ipynb#ch0000004?line=9'>10</a>\u001b[0m     eval_data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([encoder(d) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m eval_set_images], \u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nsde/Documents/LaplaceAE/notebooks/semi.ipynb#ch0000004?line=10'>11</a>\u001b[0m     res_mean\u001b[39m.\u001b[39mappend([ ])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_encoding_ae' is not defined"
     ]
    }
   ],
   "source": [
    "z_grid = torch.meshgrid([torch.linspace(-20, 20, 200), torch.linspace(-20, 20, 200)])\n",
    "z_grid = torch.stack(z_grid, 0).reshape(2, -1).T\n",
    "\n",
    "names = ['ae', 'mcae', 'vae', 'lae']\n",
    "scales = [1, 2, 5, 10, 20]\n",
    "fig, ax = plt.subplots(4, len(scales), figsize=(20, 10))\n",
    "res_mean, res_std = [], []\n",
    "reps = 5\n",
    "for i, (name, encoder) in enumerate(zip(names, [get_encoding_ae, get_encoding_mcae, get_encoding_vae, get_encoding_lae])):\n",
    "    eval_data = torch.cat([encoder(d) for d in eval_set_images], 0)\n",
    "    res_mean.append([ ])\n",
    "    res_std.append([ ])\n",
    "    for j, s in enumerate(scales):\n",
    "        print(encoder, s)\n",
    "        res = [ ]\n",
    "        for r in range(reps):\n",
    "            train_data = torch.cat([encoder(d, s) for d in selected_images], 0)\n",
    "            train_labels = selected_labels.repeat_interleave(s) if name != 'ae' else selected_labels\n",
    "            #classifier = KNeighborsClassifier(n_neighbors=2)\n",
    "            classifier = GridSearchCV(\n",
    "                KNeighborsClassifier(),\n",
    "                {'n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9, 10]},\n",
    "                cv=3,\n",
    "                refit=True,\n",
    "            )\n",
    "            classifier.fit(train_data.detach().numpy(), train_labels.detach().numpy())\n",
    "            preds = classifier.predict(eval_data.detach().numpy())\n",
    "            acc = (preds == eval_set_labels.numpy()).mean()\n",
    "            res.append(acc)\n",
    "\n",
    "        res_mean[-1].append(np.mean(res))\n",
    "        res_std[-1].append(np.std(res))\n",
    "\n",
    "        preds_grid = classifier.predict(z_grid.numpy())\n",
    "        ax[i, j].contourf(\n",
    "            z_grid[:,0].reshape(100, 100),\n",
    "            z_grid[:,1].reshape(100, 100),\n",
    "            preds_grid.reshape(100, 100)\n",
    "            alpha=0.1\n",
    "        )\n",
    "        ax[i, j].scatter(eval_data[:,0], eval_data[:, 1], c=eval_set_labels)\n",
    "res_mean = np.asarray(res_mean)\n",
    "res_std = np.asarray(res_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1c6d0a98250>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "for i, name in enumerate(names):\n",
    "    plt.plot(scales, res_mean[i], label=name)\n",
    "    plt.fill_between(scales, res_mean[i] + res_std[i], res_mean[i] - res_std[i], alpha=0.5)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7353333333333333, 0.7303333333333333, 0.811, 0.819, 0.8053333333333335], [0.8436666666666667, 0.8450000000000001, 0.8583333333333333, 0.8196666666666665, 0.8113333333333334], [0.8850000000000001, 0.8403333333333333, 0.8373333333333334, 0.8096666666666666, 0.8103333333333333], [0.9053333333333334, 0.9166666666666666, 0.9136666666666667, 0.8636666666666667, 0.88], [0.859, 0.757, 0.8559999999999999, 0.8553333333333333, 0.8396666666666667]]\n",
      "[[0.07352248333370928, 0.0634577724860311, 0.007118052168020829, 0.007788880963698574, 0.00368178700572909], [0.039684869772860384, 0.024711670657134206, 0.007845734863959886, 0.006548960901462796, 0.00464279609239466], [0.010801234497346443, 0.01405544576153869, 0.016213848676020427, 0.037809463835864615, 0.00543650214343333], [0.022216110270602198, 0.0026246692913372725, 0.0024944382578492965, 0.017133463034528545, 0.02177154105707726], [0.029631064780058135, 0.052198339692624954, 0.018493242008906945, 0.008993825042154702, 0.004496912521077351]]\n"
     ]
    }
   ],
   "source": [
    "res_m, res_s = [ ], [ ]\n",
    "base_path = \"..\\weights\\mnist\\lae_model_selection_7_may\\weights_mnist_model_selection\"\n",
    "for folder in os.listdir(base_path):\n",
    "    res_m.append([ ])\n",
    "    res_s.append([ ])\n",
    "    with open(f\"{base_path}/{folder}/config.yaml\") as file:\n",
    "        config = yaml.full_load(file)\n",
    "    lae_encoder = get_encoder(config, latent_size=2)\n",
    "    lae_decoder = get_decoder(config, latent_size=2)\n",
    "    lae_net = get_model(lae_encoder, lae_decoder).eval()\n",
    "    lae_net.load_state_dict(torch.load(f\"{base_path}/{folder}/net.pth\"))\n",
    "    hessian_approx = (\n",
    "        sampler.BlockSampler()\n",
    "        if config[\"approximation\"] == \"block\"\n",
    "        else sampler.DiagSampler()\n",
    "    )\n",
    "    h = torch.load(f\"{base_path}/{folder}/hessian.pth\")\n",
    "    sigma_q = hessian_approx.invert(h).cpu()\n",
    "\n",
    "    def get_encoding_lae(data, n=1, layer=5):\n",
    "        embeddings = [ ]\n",
    "        def fw_hook_get_latent(module, input, output):\n",
    "            embeddings.append(output.detach().cpu())\n",
    "        temp = deepcopy(lae_net)\n",
    "        temp[layer].register_forward_hook(fw_hook_get_latent)\n",
    "\n",
    "        mu_q = parameters_to_vector(temp.parameters())\n",
    "        sample = hessian_approx.sample(mu_q, sigma_q, n_samples=n)\n",
    "        for i in range(n):\n",
    "            vector_to_parameters(sample[i], temp.parameters())\n",
    "            _ = temp(data)\n",
    "        return torch.cat(embeddings, 0)\n",
    "\n",
    "    eval_data = torch.cat([get_encoding_lae(d.unsqueeze(0)) for d in eval_set_images], 0)\n",
    "    for s in [1, 2, 5, 10, 20]:\n",
    "        res = [ ]\n",
    "        for r in range(3):\n",
    "            train_data = torch.cat([get_encoding_lae(d.unsqueeze(0), s) for d in selected_images], 0)\n",
    "            train_labels = selected_labels.repeat_interleave(s)\n",
    "            classifier = GridSearchCV(\n",
    "                KNeighborsClassifier(),\n",
    "                {'n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9, 10]},\n",
    "                cv=2,\n",
    "                refit=True,\n",
    "            )\n",
    "            classifier.fit(train_data.detach().numpy(), train_labels.detach().numpy())\n",
    "            preds = classifier.predict(eval_data.detach().numpy())\n",
    "            acc = (preds == eval_set_labels.numpy()).mean()\n",
    "            res.append(acc)\n",
    "        res_m[-1].append(np.mean(res))\n",
    "        res_s[-1].append(np.std(res))\n",
    "\n",
    "print(res_m)\n",
    "print(res_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2062, 0.6500],\n",
       "        [1.2406, 0.7277],\n",
       "        [1.2293, 0.7060],\n",
       "        [1.1829, 0.6542],\n",
       "        [1.2076, 0.6943],\n",
       "        [1.2203, 0.7586],\n",
       "        [1.1422, 0.6292],\n",
       "        [1.1810, 0.7095],\n",
       "        [1.1409, 0.7560],\n",
       "        [1.2680, 0.6393]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_encoding_lae(selected_images[0].unsqueeze(0), s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "624dc3edb398ccd32d6502e59fa57cd42d17080308deeb00d3a732b8d526ac16"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('laplace')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
