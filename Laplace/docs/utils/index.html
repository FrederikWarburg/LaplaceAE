<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>laplace.utils API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>laplace.utils</code></h1>
</header>
<section id="section-intro">
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="laplace.utils.feature_extractor" href="feature_extractor.html">laplace.utils.feature_extractor</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="laplace.utils.matrix" href="matrix.html">laplace.utils.matrix</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="laplace.utils.subnetmask" href="subnetmask.html">laplace.utils.subnetmask</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="laplace.utils.swag" href="swag.html">laplace.utils.swag</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="laplace.utils.utils" href="utils.html">laplace.utils.utils</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="laplace.utils.get_nll"><code class="name flex">
<span>def <span class="ident">get_nll</span></span>(<span>out_dist, targets)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="laplace.utils.validate"><code class="name flex">
<span>def <span class="ident">validate</span></span>(<span>laplace, val_loader, pred_type='glm', link_approx='probit', n_samples=100)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="laplace.utils.parameters_per_layer"><code class="name flex">
<span>def <span class="ident">parameters_per_layer</span></span>(<span>model)</span>
</code></dt>
<dd>
<div class="desc"><p>Get number of parameters per layer.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>torch.nn.Module</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>params_per_layer</code></strong> :&ensp;<code>list[int]</code></dt>
<dd>&nbsp;</dd>
</dl></div>
</dd>
<dt id="laplace.utils.invsqrt_precision"><code class="name flex">
<span>def <span class="ident">invsqrt_precision</span></span>(<span>M)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute <code>M^{-0.5}</code> as a tridiagonal matrix.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>M</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>M_invsqrt</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>&nbsp;</dd>
</dl></div>
</dd>
<dt id="laplace.utils.kron"><code class="name flex">
<span>def <span class="ident">kron</span></span>(<span>t1, t2)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the Kronecker product between two tensors.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>t1</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>t2</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>kron_product</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>&nbsp;</dd>
</dl></div>
</dd>
<dt id="laplace.utils.diagonal_add_scalar"><code class="name flex">
<span>def <span class="ident">diagonal_add_scalar</span></span>(<span>X, value)</span>
</code></dt>
<dd>
<div class="desc"><p>Add scalar value <code>value</code> to diagonal of <code>X</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>value</code></strong> :&ensp;<code>torch.Tensor</code> or <code>float</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>X_add_scalar</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>&nbsp;</dd>
</dl></div>
</dd>
<dt id="laplace.utils.symeig"><code class="name flex">
<span>def <span class="ident">symeig</span></span>(<span>M)</span>
</code></dt>
<dd>
<div class="desc"><p>Symetric eigendecomposition avoiding failure cases by
adding and removing jitter to the diagonal.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>M</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>L</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>eigenvalues</dd>
<dt><strong><code>W</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>eigenvectors</dd>
</dl></div>
</dd>
<dt id="laplace.utils.block_diag"><code class="name flex">
<span>def <span class="ident">block_diag</span></span>(<span>blocks)</span>
</code></dt>
<dd>
<div class="desc"><p>Compose block-diagonal matrix of individual blocks.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>blocks</code></strong> :&ensp;<code>list[torch.Tensor]</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>M</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>&nbsp;</dd>
</dl></div>
</dd>
<dt id="laplace.utils.expand_prior_precision"><code class="name flex">
<span>def <span class="ident">expand_prior_precision</span></span>(<span>prior_prec, model)</span>
</code></dt>
<dd>
<div class="desc"><p>Expand prior precision to match the shape of the model parameters.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>prior_prec</code></strong> :&ensp;<code>torch.Tensor 1-dimensional</code></dt>
<dd>prior precision</dd>
<dt><strong><code>model</code></strong> :&ensp;<code>torch.nn.Module</code></dt>
<dd>torch model with parameters that are regularized by prior_prec</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>expanded_prior_prec</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>expanded prior precision has the same shape as model parameters</dd>
</dl></div>
</dd>
<dt id="laplace.utils.fit_diagonal_swag_var"><code class="name flex">
<span>def <span class="ident">fit_diagonal_swag_var</span></span>(<span>model, train_loader, criterion, n_snapshots_total=40, snapshot_freq=1, lr=0.01, momentum=0.9, weight_decay=0.0003, min_var=1e-30)</span>
</code></dt>
<dd>
<div class="desc"><p>Fit diagonal SWAG [1], which estimates marginal variances of model parameters by
computing the first and second moment of SGD iterates with a large learning rate.</p>
<p>Implementation partly adapted from:
- <a href="https://github.com/wjmaddox/swa_gaussian/blob/master/swag/posteriors/swag.py">https://github.com/wjmaddox/swa_gaussian/blob/master/swag/posteriors/swag.py</a>
- <a href="https://github.com/wjmaddox/swa_gaussian/blob/master/experiments/train/run_swag.py">https://github.com/wjmaddox/swa_gaussian/blob/master/experiments/train/run_swag.py</a></p>
<h2 id="references">References</h2>
<p>[1] Maddox, W., Garipov, T., Izmailov, P., Vetrov, D., Wilson, AG.
<a href="https://arxiv.org/abs/1902.02476"><em>A Simple Baseline for Bayesian Uncertainty in Deep Learning</em></a>.
NeurIPS 2019.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>torch.nn.Module</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>train_loader</code></strong> :&ensp;<code>torch.data.utils.DataLoader</code></dt>
<dd>training data loader to use for snapshot collection</dd>
<dt><strong><code>criterion</code></strong> :&ensp;<code>torch.nn.CrossEntropyLoss</code> or <code>torch.nn.MSELoss</code></dt>
<dd>loss function to use for snapshot collection</dd>
<dt><strong><code>n_snapshots_total</code></strong> :&ensp;<code>int</code></dt>
<dd>total number of model snapshots to collect</dd>
<dt><strong><code>snapshot_freq</code></strong> :&ensp;<code>int</code></dt>
<dd>snapshot collection frequency (in epochs)</dd>
<dt><strong><code>lr</code></strong> :&ensp;<code>float</code></dt>
<dd>SGD learning rate for collecting snapshots</dd>
<dt><strong><code>momentum</code></strong> :&ensp;<code>float</code></dt>
<dd>SGD momentum</dd>
<dt><strong><code>weight_decay</code></strong> :&ensp;<code>float</code></dt>
<dd>SGD weight decay</dd>
<dt><strong><code>min_var</code></strong> :&ensp;<code>float</code></dt>
<dd>minimum parameter variance to clamp to (for numerical stability)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>param_variances</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>vector of marginal variances for each model parameter</dd>
</dl></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="laplace.utils.FeatureExtractor"><code class="flex name class">
<span>class <span class="ident">FeatureExtractor</span></span>
<span>(</span><span>model: torch.nn.modules.module.Module, last_layer_name: Optional[str] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Feature extractor for a PyTorch neural network.
A wrapper which can return the output of the penultimate layer in addition to
the output of the last layer for each forward pass. If the name of the last
layer is not known, it can determine it automatically. It assumes that the
last layer is linear and that for every forward pass the last layer is the same.
If the name of the last layer is known, it can be passed as a parameter at
initilization; this is the safest way to use this class.
Based on <a href="https://gist.github.com/fkodom/27ed045c9051a39102e8bcf4ce31df76.">https://gist.github.com/fkodom/27ed045c9051a39102e8bcf4ce31df76.</a></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>torch.nn.Module</code></dt>
<dd>PyTorch model</dd>
<dt><strong><code>last_layer_name</code></strong> :&ensp;<code>str</code>, default=<code>None</code></dt>
<dd>if the name of the last layer is already known, otherwise it will
be determined automatically.</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="laplace.utils.FeatureExtractor.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="laplace.utils.FeatureExtractor.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="laplace.utils.FeatureExtractor.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Forward pass. If the last layer is not known yet, it will be
determined when this function is called for the first time.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>one batch of data to use as input for the forward pass</dd>
</dl></div>
</dd>
<dt id="laplace.utils.FeatureExtractor.forward_with_features"><code class="name flex">
<span>def <span class="ident">forward_with_features</span></span>(<span>self, x: torch.Tensor) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Forward pass which returns the output of the penultimate layer along
with the output of the last layer. If the last layer is not known yet,
it will be determined when this function is called for the first time.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>one batch of data to use as input for the forward pass</dd>
</dl></div>
</dd>
<dt id="laplace.utils.FeatureExtractor.set_last_layer"><code class="name flex">
<span>def <span class="ident">set_last_layer</span></span>(<span>self, last_layer_name: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Set the last layer of the model by its name. This sets the forward
hook to get the output of the penultimate layer.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>last_layer_name</code></strong> :&ensp;<code>str</code></dt>
<dd>the name of the last layer (fixed in <code>model.named_modules()</code>).</dd>
</dl></div>
</dd>
<dt id="laplace.utils.FeatureExtractor.find_last_layer"><code class="name flex">
<span>def <span class="ident">find_last_layer</span></span>(<span>self, x: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Automatically determines the last layer of the model with one
forward pass. It assumes that the last layer is the same for every
forward pass and that it is an instance of <code>torch.nn.Linear</code>.
Might not work with every architecture, but is tested with all PyTorch
torchvision classification models (besides SqueezeNet, which has no
linear last layer).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>one batch of data to use as input for the forward pass</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="laplace.utils.Kron"><code class="flex name class">
<span>class <span class="ident">Kron</span></span>
<span>(</span><span>kfacs)</span>
</code></dt>
<dd>
<div class="desc"><p>Kronecker factored approximate curvature representation for a corresponding
neural network.
Each element in <code>kfacs</code> is either a tuple or single matrix.
A tuple represents two Kronecker factors <span><span class="MathJax_Preview">Q</span><script type="math/tex">Q</script></span>, and <span><span class="MathJax_Preview">H</span><script type="math/tex">H</script></span> and a single element
is just a full block Hessian approximation.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>kfacs</code></strong> :&ensp;<code>list[Tuple]</code></dt>
<dd>each element in the list is a Tuple of two Kronecker factors Q, H
or a single matrix approximating the Hessian (in case of bias, for example)</dd>
</dl></div>
<h3>Static methods</h3>
<dl>
<dt id="laplace.utils.Kron.init_from_model"><code class="name flex">
<span>def <span class="ident">init_from_model</span></span>(<span>model, device)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize Kronecker factors based on a models architecture.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>torch.nn.Module</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>torch.device</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>kron</code></strong> :&ensp;<code><a title="laplace.utils.Kron" href="#laplace.utils.Kron">Kron</a></code></dt>
<dd>&nbsp;</dd>
</dl></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="laplace.utils.Kron.decompose"><code class="name flex">
<span>def <span class="ident">decompose</span></span>(<span>self, damping=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Eigendecompose Kronecker factors and turn into <code><a title="laplace.utils.KronDecomposed" href="#laplace.utils.KronDecomposed">KronDecomposed</a></code>.
Parameters</p>
<hr>
<dl>
<dt><strong><code>damping</code></strong> :&ensp;<code>bool</code></dt>
<dd>use damping</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>kron_decomposed</code></strong> :&ensp;<code><a title="laplace.utils.KronDecomposed" href="#laplace.utils.KronDecomposed">KronDecomposed</a></code></dt>
<dd>&nbsp;</dd>
</dl></div>
</dd>
<dt id="laplace.utils.Kron.bmm"><code class="name flex">
<span>def <span class="ident">bmm</span></span>(<span>self, W: torch.Tensor, exponent: float = 1) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Batched matrix multiplication with the Kronecker factors.
If Kron is <code>H</code>, we compute <code>H @ W</code>.
This is useful for computing the predictive or a regularization
based on Kronecker factors as in continual learning.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>W</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>matrix <code>(batch, classes, params)</code></dd>
<dt><strong><code>exponent</code></strong> :&ensp;<code>float</code>, default=<code>1</code></dt>
<dd>only can be <code>1</code> for Kron, requires <code><a title="laplace.utils.KronDecomposed" href="#laplace.utils.KronDecomposed">KronDecomposed</a></code> for other
exponent values of the Kronecker factors.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>SW</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>result <code>(batch, classes, params)</code></dd>
</dl></div>
</dd>
<dt id="laplace.utils.Kron.logdet"><code class="name flex">
<span>def <span class="ident">logdet</span></span>(<span>self) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Compute log determinant of the Kronecker factors and sums them up.
This corresponds to the log determinant of the entire Hessian approximation.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>logdet</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>&nbsp;</dd>
</dl></div>
</dd>
<dt id="laplace.utils.Kron.diag"><code class="name flex">
<span>def <span class="ident">diag</span></span>(<span>self) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Extract diagonal of the entire Kronecker factorization.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>diag</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>&nbsp;</dd>
</dl></div>
</dd>
<dt id="laplace.utils.Kron.to_matrix"><code class="name flex">
<span>def <span class="ident">to_matrix</span></span>(<span>self) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Make the Kronecker factorization dense by computing the kronecker product.
Warning: this should only be used for testing purposes as it will allocate
large amounts of memory for big architectures.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>block_diag</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>&nbsp;</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="laplace.utils.KronDecomposed"><code class="flex name class">
<span>class <span class="ident">KronDecomposed</span></span>
<span>(</span><span>eigenvectors, eigenvalues, deltas=None, damping=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Decomposed Kronecker factored approximate curvature representation
for a corresponding neural network.
Each matrix in <code><a title="laplace.utils.Kron" href="#laplace.utils.Kron">Kron</a></code> is decomposed to obtain <code><a title="laplace.utils.KronDecomposed" href="#laplace.utils.KronDecomposed">KronDecomposed</a></code>.
Front-loading decomposition allows cheap repeated computation
of inverses and log determinants.
In contrast to <code><a title="laplace.utils.Kron" href="#laplace.utils.Kron">Kron</a></code>, we can add scalar or layerwise scalars but
we cannot add other <code><a title="laplace.utils.Kron" href="#laplace.utils.Kron">Kron</a></code> or <code><a title="laplace.utils.KronDecomposed" href="#laplace.utils.KronDecomposed">KronDecomposed</a></code> anymore.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>eigenvectors</code></strong> :&ensp;<code>list[Tuple[torch.Tensor]]</code></dt>
<dd>eigenvectors corresponding to matrices in a corresponding <code><a title="laplace.utils.Kron" href="#laplace.utils.Kron">Kron</a></code></dd>
<dt><strong><code>eigenvalues</code></strong> :&ensp;<code>list[Tuple[torch.Tensor]]</code></dt>
<dd>eigenvalues corresponding to matrices in a corresponding <code><a title="laplace.utils.Kron" href="#laplace.utils.Kron">Kron</a></code></dd>
<dt><strong><code>deltas</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>addend for each group of Kronecker factors representing, for example,
a prior precision</dd>
<dt><strong><code>dampen</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>use dampen approximation mixing prior and Kron partially multiplicatively</dd>
</dl></div>
<h3>Methods</h3>
<dl>
<dt id="laplace.utils.KronDecomposed.detach"><code class="name flex">
<span>def <span class="ident">detach</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="laplace.utils.KronDecomposed.logdet"><code class="name flex">
<span>def <span class="ident">logdet</span></span>(<span>self) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Compute log determinant of the Kronecker factors and sums them up.
This corresponds to the log determinant of the entire Hessian approximation.
In contrast to <code><a title="laplace.utils.Kron.logdet" href="#laplace.utils.Kron.logdet">Kron.logdet()</a></code>, additive <code>deltas</code> corresponding to prior
precisions are added.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>logdet</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>&nbsp;</dd>
</dl></div>
</dd>
<dt id="laplace.utils.KronDecomposed.inv_square_form"><code class="name flex">
<span>def <span class="ident">inv_square_form</span></span>(<span>self, W: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="laplace.utils.KronDecomposed.bmm"><code class="name flex">
<span>def <span class="ident">bmm</span></span>(<span>self, W: torch.Tensor, exponent: float = -1) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Batched matrix multiplication with the decomposed Kronecker factors.
This is useful for computing the predictive or a regularization loss.
Compared to <code><a title="laplace.utils.Kron.bmm" href="#laplace.utils.Kron.bmm">Kron.bmm()</a></code>, a prior can be added here in form of <code>deltas</code>
and the exponent can be other than just 1.
Computes <span><span class="MathJax_Preview">H^{exponent} W</span><script type="math/tex">H^{exponent} W</script></span>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>W</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>matrix <code>(batch, classes, params)</code></dd>
<dt><strong><code>exponent</code></strong> :&ensp;<code>float</code>, default=<code>1</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>SW</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>result <code>(batch, classes, params)</code></dd>
</dl></div>
</dd>
<dt id="laplace.utils.KronDecomposed.to_matrix"><code class="name flex">
<span>def <span class="ident">to_matrix</span></span>(<span>self, exponent: float = 1) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Make the Kronecker factorization dense by computing the kronecker product.
Warning: this should only be used for testing purposes as it will allocate
large amounts of memory for big architectures.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>block_diag</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>&nbsp;</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="laplace.utils.SubnetMask"><code class="flex name class">
<span>class <span class="ident">SubnetMask</span></span>
<span>(</span><span>model)</span>
</code></dt>
<dd>
<div class="desc"><p>Baseclass for all subnetwork masks in this library (for subnetwork Laplace).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>torch.nn.Module</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="laplace.utils.subnetmask.ModuleNameSubnetMask" href="subnetmask.html#laplace.utils.subnetmask.ModuleNameSubnetMask">ModuleNameSubnetMask</a></li>
<li><a title="laplace.utils.subnetmask.ParamNameSubnetMask" href="subnetmask.html#laplace.utils.subnetmask.ParamNameSubnetMask">ParamNameSubnetMask</a></li>
<li>laplace.utils.subnetmask.ScoreBasedSubnetMask</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="laplace.utils.SubnetMask.indices"><code class="name">var <span class="ident">indices</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="laplace.utils.SubnetMask.n_params_subnet"><code class="name">var <span class="ident">n_params_subnet</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="laplace.utils.SubnetMask.convert_subnet_mask_to_indices"><code class="name flex">
<span>def <span class="ident">convert_subnet_mask_to_indices</span></span>(<span>self, subnet_mask)</span>
</code></dt>
<dd>
<div class="desc"><p>Converts a subnetwork mask into subnetwork indices.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>subnet_mask</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>a binary vector of size (n_params) where 1s locate the subnetwork parameters
within the vectorized model parameters
(i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>subnet_mask_indices</code></strong> :&ensp;<code>torch.LongTensor</code></dt>
<dd>a vector of indices of the vectorized model parameters
(i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>)
that define the subnetwork</dd>
</dl></div>
</dd>
<dt id="laplace.utils.SubnetMask.select"><code class="name flex">
<span>def <span class="ident">select</span></span>(<span>self, train_loader=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Select the subnetwork mask.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>train_loader</code></strong> :&ensp;<code>torch.data.utils.DataLoader</code>, default=<code>None</code></dt>
<dd>each iterate is a training batch (X, y);
<code>train_loader.dataset</code> needs to be set to access <span><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span>, size of the data set</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>subnet_mask_indices</code></strong> :&ensp;<code>torch.LongTensor</code></dt>
<dd>a vector of indices of the vectorized model parameters
(i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>)
that define the subnetwork</dd>
</dl></div>
</dd>
<dt id="laplace.utils.SubnetMask.get_subnet_mask"><code class="name flex">
<span>def <span class="ident">get_subnet_mask</span></span>(<span>self, train_loader)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the subnetwork mask.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>train_loader</code></strong> :&ensp;<code>torch.data.utils.DataLoader</code></dt>
<dd>each iterate is a training batch (X, y);
<code>train_loader.dataset</code> needs to be set to access <span><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span>, size of the data set</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>subnet_mask</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>a binary vector of size (n_params) where 1s locate the subnetwork parameters
within the vectorized model parameters
(i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>)</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="laplace.utils.RandomSubnetMask"><code class="flex name class">
<span>class <span class="ident">RandomSubnetMask</span></span>
<span>(</span><span>model, n_params_subnet)</span>
</code></dt>
<dd>
<div class="desc"><p>Subnetwork mask of parameters sampled uniformly at random.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>laplace.utils.subnetmask.ScoreBasedSubnetMask</li>
<li><a title="laplace.utils.subnetmask.SubnetMask" href="subnetmask.html#laplace.utils.subnetmask.SubnetMask">SubnetMask</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="laplace.utils.RandomSubnetMask.compute_param_scores"><code class="name flex">
<span>def <span class="ident">compute_param_scores</span></span>(<span>self, train_loader)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="laplace.utils.subnetmask.SubnetMask" href="subnetmask.html#laplace.utils.subnetmask.SubnetMask">SubnetMask</a></b></code>:
<ul class="hlist">
<li><code><a title="laplace.utils.subnetmask.SubnetMask.convert_subnet_mask_to_indices" href="subnetmask.html#laplace.utils.subnetmask.SubnetMask.convert_subnet_mask_to_indices">convert_subnet_mask_to_indices</a></code></li>
<li><code><a title="laplace.utils.subnetmask.SubnetMask.get_subnet_mask" href="subnetmask.html#laplace.utils.subnetmask.SubnetMask.get_subnet_mask">get_subnet_mask</a></code></li>
<li><code><a title="laplace.utils.subnetmask.SubnetMask.select" href="subnetmask.html#laplace.utils.subnetmask.SubnetMask.select">select</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="laplace.utils.LargestMagnitudeSubnetMask"><code class="flex name class">
<span>class <span class="ident">LargestMagnitudeSubnetMask</span></span>
<span>(</span><span>model, n_params_subnet)</span>
</code></dt>
<dd>
<div class="desc"><p>Subnetwork mask identifying the parameters with the largest magnitude.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>laplace.utils.subnetmask.ScoreBasedSubnetMask</li>
<li><a title="laplace.utils.subnetmask.SubnetMask" href="subnetmask.html#laplace.utils.subnetmask.SubnetMask">SubnetMask</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="laplace.utils.LargestMagnitudeSubnetMask.compute_param_scores"><code class="name flex">
<span>def <span class="ident">compute_param_scores</span></span>(<span>self, train_loader)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="laplace.utils.subnetmask.SubnetMask" href="subnetmask.html#laplace.utils.subnetmask.SubnetMask">SubnetMask</a></b></code>:
<ul class="hlist">
<li><code><a title="laplace.utils.subnetmask.SubnetMask.convert_subnet_mask_to_indices" href="subnetmask.html#laplace.utils.subnetmask.SubnetMask.convert_subnet_mask_to_indices">convert_subnet_mask_to_indices</a></code></li>
<li><code><a title="laplace.utils.subnetmask.SubnetMask.get_subnet_mask" href="subnetmask.html#laplace.utils.subnetmask.SubnetMask.get_subnet_mask">get_subnet_mask</a></code></li>
<li><code><a title="laplace.utils.subnetmask.SubnetMask.select" href="subnetmask.html#laplace.utils.subnetmask.SubnetMask.select">select</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="laplace.utils.LargestVarianceDiagLaplaceSubnetMask"><code class="flex name class">
<span>class <span class="ident">LargestVarianceDiagLaplaceSubnetMask</span></span>
<span>(</span><span>model, n_params_subnet, diag_laplace_model)</span>
</code></dt>
<dd>
<div class="desc"><p>Subnetwork mask identifying the parameters with the largest marginal variances
(estimated using a diagonal Laplace approximation over all model parameters).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>torch.nn.Module</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>n_params_subnet</code></strong> :&ensp;<code>int</code></dt>
<dd>number of parameters in the subnetwork (i.e. number of top-scoring parameters to select)</dd>
<dt><strong><code>diag_laplace_model</code></strong> :&ensp;<code><a title="laplace.baselaplace.DiagLaplace" href="../baselaplace.html#laplace.baselaplace.DiagLaplace">DiagLaplace</a></code></dt>
<dd>diagonal Laplace model to use for variance estimation</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>laplace.utils.subnetmask.ScoreBasedSubnetMask</li>
<li><a title="laplace.utils.subnetmask.SubnetMask" href="subnetmask.html#laplace.utils.subnetmask.SubnetMask">SubnetMask</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="laplace.utils.LargestVarianceDiagLaplaceSubnetMask.compute_param_scores"><code class="name flex">
<span>def <span class="ident">compute_param_scores</span></span>(<span>self, train_loader)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="laplace.utils.subnetmask.SubnetMask" href="subnetmask.html#laplace.utils.subnetmask.SubnetMask">SubnetMask</a></b></code>:
<ul class="hlist">
<li><code><a title="laplace.utils.subnetmask.SubnetMask.convert_subnet_mask_to_indices" href="subnetmask.html#laplace.utils.subnetmask.SubnetMask.convert_subnet_mask_to_indices">convert_subnet_mask_to_indices</a></code></li>
<li><code><a title="laplace.utils.subnetmask.SubnetMask.get_subnet_mask" href="subnetmask.html#laplace.utils.subnetmask.SubnetMask.get_subnet_mask">get_subnet_mask</a></code></li>
<li><code><a title="laplace.utils.subnetmask.SubnetMask.select" href="subnetmask.html#laplace.utils.subnetmask.SubnetMask.select">select</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="laplace.utils.LargestVarianceSWAGSubnetMask"><code class="flex name class">
<span>class <span class="ident">LargestVarianceSWAGSubnetMask</span></span>
<span>(</span><span>model, n_params_subnet, likelihood='classification', swag_n_snapshots=40, swag_snapshot_freq=1, swag_lr=0.01)</span>
</code></dt>
<dd>
<div class="desc"><p>Subnetwork mask identifying the parameters with the largest marginal variances
(estimated using diagonal SWAG over all model parameters).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>torch.nn.Module</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>n_params_subnet</code></strong> :&ensp;<code>int</code></dt>
<dd>number of parameters in the subnetwork (i.e. number of top-scoring parameters to select)</dd>
<dt><strong><code>likelihood</code></strong> :&ensp;<code>str</code></dt>
<dd>'classification' or 'regression'</dd>
<dt><strong><code>swag_n_snapshots</code></strong> :&ensp;<code>int</code></dt>
<dd>number of model snapshots to collect for SWAG</dd>
<dt><strong><code>swag_snapshot_freq</code></strong> :&ensp;<code>int</code></dt>
<dd>SWAG snapshot collection frequency (in epochs)</dd>
<dt><strong><code>swag_lr</code></strong> :&ensp;<code>float</code></dt>
<dd>learning rate for SWAG snapshot collection</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>laplace.utils.subnetmask.ScoreBasedSubnetMask</li>
<li><a title="laplace.utils.subnetmask.SubnetMask" href="subnetmask.html#laplace.utils.subnetmask.SubnetMask">SubnetMask</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="laplace.utils.LargestVarianceSWAGSubnetMask.compute_param_scores"><code class="name flex">
<span>def <span class="ident">compute_param_scores</span></span>(<span>self, train_loader)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="laplace.utils.subnetmask.SubnetMask" href="subnetmask.html#laplace.utils.subnetmask.SubnetMask">SubnetMask</a></b></code>:
<ul class="hlist">
<li><code><a title="laplace.utils.subnetmask.SubnetMask.convert_subnet_mask_to_indices" href="subnetmask.html#laplace.utils.subnetmask.SubnetMask.convert_subnet_mask_to_indices">convert_subnet_mask_to_indices</a></code></li>
<li><code><a title="laplace.utils.subnetmask.SubnetMask.get_subnet_mask" href="subnetmask.html#laplace.utils.subnetmask.SubnetMask.get_subnet_mask">get_subnet_mask</a></code></li>
<li><code><a title="laplace.utils.subnetmask.SubnetMask.select" href="subnetmask.html#laplace.utils.subnetmask.SubnetMask.select">select</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="laplace.utils.ParamNameSubnetMask"><code class="flex name class">
<span>class <span class="ident">ParamNameSubnetMask</span></span>
<span>(</span><span>model, parameter_names)</span>
</code></dt>
<dd>
<div class="desc"><p>Subnetwork mask corresponding to the specified parameters of the neural network.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>torch.nn.Module</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>parameter_names</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>list of names of the parameters (as in <code>model.named_parameters()</code>)
that define the subnetwork</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="laplace.utils.subnetmask.SubnetMask" href="subnetmask.html#laplace.utils.subnetmask.SubnetMask">SubnetMask</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="laplace.utils.ParamNameSubnetMask.get_subnet_mask"><code class="name flex">
<span>def <span class="ident">get_subnet_mask</span></span>(<span>self, train_loader)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the subnetwork mask identifying the specified parameters.</p></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="laplace.utils.subnetmask.SubnetMask" href="subnetmask.html#laplace.utils.subnetmask.SubnetMask">SubnetMask</a></b></code>:
<ul class="hlist">
<li><code><a title="laplace.utils.subnetmask.SubnetMask.convert_subnet_mask_to_indices" href="subnetmask.html#laplace.utils.subnetmask.SubnetMask.convert_subnet_mask_to_indices">convert_subnet_mask_to_indices</a></code></li>
<li><code><a title="laplace.utils.subnetmask.SubnetMask.select" href="subnetmask.html#laplace.utils.subnetmask.SubnetMask.select">select</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="laplace.utils.ModuleNameSubnetMask"><code class="flex name class">
<span>class <span class="ident">ModuleNameSubnetMask</span></span>
<span>(</span><span>model, module_names)</span>
</code></dt>
<dd>
<div class="desc"><p>Subnetwork mask corresponding to the specified modules of the neural network.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>torch.nn.Module</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>parameter_names</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>list of names of the modules (as in <code>model.named_modules()</code>) that define the subnetwork;
the modules cannot have children, i.e. need to be leaf modules</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="laplace.utils.subnetmask.SubnetMask" href="subnetmask.html#laplace.utils.subnetmask.SubnetMask">SubnetMask</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="laplace.utils.subnetmask.LastLayerSubnetMask" href="subnetmask.html#laplace.utils.subnetmask.LastLayerSubnetMask">LastLayerSubnetMask</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="laplace.utils.ModuleNameSubnetMask.get_subnet_mask"><code class="name flex">
<span>def <span class="ident">get_subnet_mask</span></span>(<span>self, train_loader)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the subnetwork mask identifying the specified modules.</p></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="laplace.utils.subnetmask.SubnetMask" href="subnetmask.html#laplace.utils.subnetmask.SubnetMask">SubnetMask</a></b></code>:
<ul class="hlist">
<li><code><a title="laplace.utils.subnetmask.SubnetMask.convert_subnet_mask_to_indices" href="subnetmask.html#laplace.utils.subnetmask.SubnetMask.convert_subnet_mask_to_indices">convert_subnet_mask_to_indices</a></code></li>
<li><code><a title="laplace.utils.subnetmask.SubnetMask.select" href="subnetmask.html#laplace.utils.subnetmask.SubnetMask.select">select</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="laplace.utils.LastLayerSubnetMask"><code class="flex name class">
<span>class <span class="ident">LastLayerSubnetMask</span></span>
<span>(</span><span>model, last_layer_name=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Subnetwork mask corresponding to the last layer of the neural network.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>torch.nn.Module</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>last_layer_name</code></strong> :&ensp;<code>str</code>, default=<code>None</code></dt>
<dd>name of the model's last layer, if None it will be determined automatically</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="laplace.utils.subnetmask.ModuleNameSubnetMask" href="subnetmask.html#laplace.utils.subnetmask.ModuleNameSubnetMask">ModuleNameSubnetMask</a></li>
<li><a title="laplace.utils.subnetmask.SubnetMask" href="subnetmask.html#laplace.utils.subnetmask.SubnetMask">SubnetMask</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="laplace.utils.LastLayerSubnetMask.get_subnet_mask"><code class="name flex">
<span>def <span class="ident">get_subnet_mask</span></span>(<span>self, train_loader)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the subnetwork mask identifying the last layer.</p></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="laplace.utils.subnetmask.ModuleNameSubnetMask" href="subnetmask.html#laplace.utils.subnetmask.ModuleNameSubnetMask">ModuleNameSubnetMask</a></b></code>:
<ul class="hlist">
<li><code><a title="laplace.utils.subnetmask.ModuleNameSubnetMask.convert_subnet_mask_to_indices" href="subnetmask.html#laplace.utils.subnetmask.SubnetMask.convert_subnet_mask_to_indices">convert_subnet_mask_to_indices</a></code></li>
<li><code><a title="laplace.utils.subnetmask.ModuleNameSubnetMask.select" href="subnetmask.html#laplace.utils.subnetmask.SubnetMask.select">select</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="laplace" href="../index.html">laplace</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="laplace.utils.feature_extractor" href="feature_extractor.html">laplace.utils.feature_extractor</a></code></li>
<li><code><a title="laplace.utils.matrix" href="matrix.html">laplace.utils.matrix</a></code></li>
<li><code><a title="laplace.utils.subnetmask" href="subnetmask.html">laplace.utils.subnetmask</a></code></li>
<li><code><a title="laplace.utils.swag" href="swag.html">laplace.utils.swag</a></code></li>
<li><code><a title="laplace.utils.utils" href="utils.html">laplace.utils.utils</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="laplace.utils.get_nll" href="#laplace.utils.get_nll">get_nll</a></code></li>
<li><code><a title="laplace.utils.validate" href="#laplace.utils.validate">validate</a></code></li>
<li><code><a title="laplace.utils.parameters_per_layer" href="#laplace.utils.parameters_per_layer">parameters_per_layer</a></code></li>
<li><code><a title="laplace.utils.invsqrt_precision" href="#laplace.utils.invsqrt_precision">invsqrt_precision</a></code></li>
<li><code><a title="laplace.utils.kron" href="#laplace.utils.kron">kron</a></code></li>
<li><code><a title="laplace.utils.diagonal_add_scalar" href="#laplace.utils.diagonal_add_scalar">diagonal_add_scalar</a></code></li>
<li><code><a title="laplace.utils.symeig" href="#laplace.utils.symeig">symeig</a></code></li>
<li><code><a title="laplace.utils.block_diag" href="#laplace.utils.block_diag">block_diag</a></code></li>
<li><code><a title="laplace.utils.expand_prior_precision" href="#laplace.utils.expand_prior_precision">expand_prior_precision</a></code></li>
<li><code><a title="laplace.utils.fit_diagonal_swag_var" href="#laplace.utils.fit_diagonal_swag_var">fit_diagonal_swag_var</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="laplace.utils.FeatureExtractor" href="#laplace.utils.FeatureExtractor">FeatureExtractor</a></code></h4>
<ul class="">
<li><code><a title="laplace.utils.FeatureExtractor.forward" href="#laplace.utils.FeatureExtractor.forward">forward</a></code></li>
<li><code><a title="laplace.utils.FeatureExtractor.forward_with_features" href="#laplace.utils.FeatureExtractor.forward_with_features">forward_with_features</a></code></li>
<li><code><a title="laplace.utils.FeatureExtractor.set_last_layer" href="#laplace.utils.FeatureExtractor.set_last_layer">set_last_layer</a></code></li>
<li><code><a title="laplace.utils.FeatureExtractor.find_last_layer" href="#laplace.utils.FeatureExtractor.find_last_layer">find_last_layer</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="laplace.utils.Kron" href="#laplace.utils.Kron">Kron</a></code></h4>
<ul class="two-column">
<li><code><a title="laplace.utils.Kron.init_from_model" href="#laplace.utils.Kron.init_from_model">init_from_model</a></code></li>
<li><code><a title="laplace.utils.Kron.decompose" href="#laplace.utils.Kron.decompose">decompose</a></code></li>
<li><code><a title="laplace.utils.Kron.bmm" href="#laplace.utils.Kron.bmm">bmm</a></code></li>
<li><code><a title="laplace.utils.Kron.logdet" href="#laplace.utils.Kron.logdet">logdet</a></code></li>
<li><code><a title="laplace.utils.Kron.diag" href="#laplace.utils.Kron.diag">diag</a></code></li>
<li><code><a title="laplace.utils.Kron.to_matrix" href="#laplace.utils.Kron.to_matrix">to_matrix</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="laplace.utils.KronDecomposed" href="#laplace.utils.KronDecomposed">KronDecomposed</a></code></h4>
<ul class="">
<li><code><a title="laplace.utils.KronDecomposed.detach" href="#laplace.utils.KronDecomposed.detach">detach</a></code></li>
<li><code><a title="laplace.utils.KronDecomposed.logdet" href="#laplace.utils.KronDecomposed.logdet">logdet</a></code></li>
<li><code><a title="laplace.utils.KronDecomposed.inv_square_form" href="#laplace.utils.KronDecomposed.inv_square_form">inv_square_form</a></code></li>
<li><code><a title="laplace.utils.KronDecomposed.bmm" href="#laplace.utils.KronDecomposed.bmm">bmm</a></code></li>
<li><code><a title="laplace.utils.KronDecomposed.to_matrix" href="#laplace.utils.KronDecomposed.to_matrix">to_matrix</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="laplace.utils.SubnetMask" href="#laplace.utils.SubnetMask">SubnetMask</a></code></h4>
<ul class="">
<li><code><a title="laplace.utils.SubnetMask.convert_subnet_mask_to_indices" href="#laplace.utils.SubnetMask.convert_subnet_mask_to_indices">convert_subnet_mask_to_indices</a></code></li>
<li><code><a title="laplace.utils.SubnetMask.select" href="#laplace.utils.SubnetMask.select">select</a></code></li>
<li><code><a title="laplace.utils.SubnetMask.get_subnet_mask" href="#laplace.utils.SubnetMask.get_subnet_mask">get_subnet_mask</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="laplace.utils.RandomSubnetMask" href="#laplace.utils.RandomSubnetMask">RandomSubnetMask</a></code></h4>
<ul class="">
<li><code><a title="laplace.utils.RandomSubnetMask.compute_param_scores" href="#laplace.utils.RandomSubnetMask.compute_param_scores">compute_param_scores</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="laplace.utils.LargestMagnitudeSubnetMask" href="#laplace.utils.LargestMagnitudeSubnetMask">LargestMagnitudeSubnetMask</a></code></h4>
<ul class="">
<li><code><a title="laplace.utils.LargestMagnitudeSubnetMask.compute_param_scores" href="#laplace.utils.LargestMagnitudeSubnetMask.compute_param_scores">compute_param_scores</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="laplace.utils.LargestVarianceDiagLaplaceSubnetMask" href="#laplace.utils.LargestVarianceDiagLaplaceSubnetMask">LargestVarianceDiagLaplaceSubnetMask</a></code></h4>
<ul class="">
<li><code><a title="laplace.utils.LargestVarianceDiagLaplaceSubnetMask.compute_param_scores" href="#laplace.utils.LargestVarianceDiagLaplaceSubnetMask.compute_param_scores">compute_param_scores</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="laplace.utils.LargestVarianceSWAGSubnetMask" href="#laplace.utils.LargestVarianceSWAGSubnetMask">LargestVarianceSWAGSubnetMask</a></code></h4>
<ul class="">
<li><code><a title="laplace.utils.LargestVarianceSWAGSubnetMask.compute_param_scores" href="#laplace.utils.LargestVarianceSWAGSubnetMask.compute_param_scores">compute_param_scores</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="laplace.utils.ParamNameSubnetMask" href="#laplace.utils.ParamNameSubnetMask">ParamNameSubnetMask</a></code></h4>
<ul class="">
<li><code><a title="laplace.utils.ParamNameSubnetMask.get_subnet_mask" href="#laplace.utils.ParamNameSubnetMask.get_subnet_mask">get_subnet_mask</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="laplace.utils.ModuleNameSubnetMask" href="#laplace.utils.ModuleNameSubnetMask">ModuleNameSubnetMask</a></code></h4>
<ul class="">
<li><code><a title="laplace.utils.ModuleNameSubnetMask.get_subnet_mask" href="#laplace.utils.ModuleNameSubnetMask.get_subnet_mask">get_subnet_mask</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="laplace.utils.LastLayerSubnetMask" href="#laplace.utils.LastLayerSubnetMask">LastLayerSubnetMask</a></code></h4>
<ul class="">
<li><code><a title="laplace.utils.LastLayerSubnetMask.get_subnet_mask" href="#laplace.utils.LastLayerSubnetMask.get_subnet_mask">get_subnet_mask</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>