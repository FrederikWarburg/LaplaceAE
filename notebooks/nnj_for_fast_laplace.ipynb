{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../stochman\")\n",
    "\n",
    "from stochman import nnj\n",
    "\n",
    "def compute_output_edge(input_edge, kernel_size=1,padding=0,stride=1,dilation=1):\n",
    "    output_edge = ( input_edge - dilation*(kernel_size-1) + 2*padding -1 )/stride +1 #output edge can be not-integer if stride!=1\n",
    "    return int(output_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "IN_c, OUT_c = 1, 1 #number of channels (for input and output)\n",
    "IN_h, IN_w = 28, 28 #number of pixels per input edges\n",
    "kernel_h, kernel_w = 3,3\n",
    "padding_h, padding_w = 1,1\n",
    "\n",
    "# some parameters to define tests\n",
    "OUT_h = compute_output_edge(IN_h, kernel_size=kernel_h, padding=padding_h, stride=1, dilation=1)\n",
    "OUT_w = compute_output_edge(IN_w, kernel_size=kernel_w, padding=padding_w, stride=1, dilation=1)\n",
    "IN_size, OUT_size = IN_c*IN_h*IN_w, OUT_c*OUT_h*OUT_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nnj.Conv2d(IN_c, \n",
    "                  OUT_c, \n",
    "                  kernel_size=(kernel_h, kernel_w), \n",
    "                  padding=(padding_h,padding_w), \n",
    "                  bias=None)\n",
    "\n",
    "feat_in = torch.zeros(batch_size, IN_c, IN_h, IN_w)\n",
    "feat_out = conv(feat_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Right and Left multiplications (J wrt to input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 784, 784]) torch.Size([2, 784, 784]) torch.Size([2, 784, 784])\n"
     ]
    }
   ],
   "source": [
    "# define a random tmp matrix\n",
    "tmp = torch.randint(0, 10, (batch_size, OUT_size, OUT_size)).type(torch.float)\n",
    "\n",
    "# compute Jt*tmp*J defining the full jacobians (correct for sure but NOT memory efficient)\n",
    "slow_Jt_tmp_J = torch.einsum(\n",
    "        'Bji,Bjk,Bkq->Biq',\n",
    "        conv._jacobian_wrt_input(feat_in, feat_out),\n",
    "        tmp,\n",
    "        conv._jacobian_wrt_input(feat_in, feat_out)\n",
    "    )\n",
    "\n",
    "# compute (Jt*tmp)*J efficiently\n",
    "fast_Jt_tmp_J_1 = conv._jacobian_wrt_input_mult_left(feat_in, feat_out,\n",
    "    conv._jacobian_wrt_input_T_mult_right(feat_in, feat_out, tmp)\n",
    ")\n",
    "\n",
    "# compute Jt*(tmp*J) efficiently\n",
    "fast_Jt_tmp_J_2 = conv._jacobian_wrt_input_T_mult_right(feat_in, feat_out,\n",
    "    conv._jacobian_wrt_input_mult_left(feat_in, feat_out, tmp)\n",
    ")\n",
    "\n",
    "# check if the shape are the same\n",
    "print(slow_Jt_tmp_J.shape, fast_Jt_tmp_J_1.shape, fast_Jt_tmp_J_2.shape)\n",
    "\n",
    "# check if the elements are the same\n",
    "assert torch.abs(torch.max(slow_Jt_tmp_J - fast_Jt_tmp_J_1)) < 1e-5\n",
    "assert torch.abs(torch.max(slow_Jt_tmp_J - fast_Jt_tmp_J_2)) < 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Right and Left multiplications (J wrt to weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 9, 784]) torch.Size([2, 9, 784])\n"
     ]
    }
   ],
   "source": [
    "# compute Jt*tmp defining the full jacobians (correct for sure but NOT memory efficient)\n",
    "slow_Jt_tmp = torch.einsum('Bji,Bjq->Biq', conv._jacobian_wrt_weight(feat_in, feat_out),tmp)\n",
    "\n",
    "if batch_size==1:\n",
    "    # compute Jt*tmp efficiently\n",
    "    fast_Jt_tmp = conv._jacobian_wrt_weight_T_mult_right(feat_in, feat_out, tmp[0], use_less_memory=False)\n",
    "\n",
    "    # check if the shape are the same\n",
    "    print(slow_Jt_tmp.shape, fast_Jt_tmp.shape)\n",
    "    # check if the elements are the same\n",
    "    assert torch.max(torch.abs(slow_Jt_tmp_J - fast_Jt_tmp_J_1)) < 1e-5\n",
    "\n",
    "else:\n",
    "    slow_Jt_tmp_sum = torch.zeros(slow_Jt_tmp.shape[1:])\n",
    "    for s in slow_Jt_tmp:\n",
    "        slow_Jt_tmp_sum =+ s\n",
    "\n",
    "    # compute Jt*tmp efficiently\n",
    "    fast_Jt_tmp = conv._jacobian_wrt_weight_T_mult_right(feat_in, feat_out, tmp, use_less_memory=True)\n",
    "\n",
    "    # check if the shape are the same\n",
    "    print(slow_Jt_tmp.shape, fast_Jt_tmp.shape)\n",
    "    # check if the elements are the same\n",
    "    assert torch.max(torch.abs(slow_Jt_tmp - fast_Jt_tmp)) < 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIAGONAL APPROXIMATION wrt input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 784]) torch.Size([2, 784]) torch.Size([2, 784])\n"
     ]
    }
   ],
   "source": [
    "diag_tmp = torch.diagonal(tmp, dim1=1, dim2=2)\n",
    "\n",
    "# SLOW METHOD\n",
    "# upscale tmp diagonal to full matrix\n",
    "tmp_simple = torch.diag_embed(diag_tmp)\n",
    "# compute J^T * tmp * J\n",
    "slow_Jt_tmp_J = torch.einsum('Bji,Bjk,Bkq->Biq', conv._jacobian_wrt_input(feat_in, feat_out), tmp_simple, conv._jacobian_wrt_input(feat_in, feat_out))\n",
    "# take only the diagonal (and discard all other elements)\n",
    "slow_diag_Jt_tmp_J = torch.diagonal(slow_Jt_tmp_J, dim1=1, dim2=2)\n",
    "\n",
    "# LESS SLOW METHOD\n",
    "# # compute J^T * tmp * J\n",
    "lessslow_Jt_tmp_J = conv._jacobian_wrt_input_sandwich_full_to_full(feat_in, feat_out, tmp_simple)\n",
    "# take only the diagonal (and discard all other elements)\n",
    "lessslow_diag_Jt_tmp_J = torch.diagonal(lessslow_Jt_tmp_J, dim1=1, dim2=2)\n",
    "\n",
    "# FAST METHOD\n",
    "fast_diag_Jt_tmp_J  = conv._jacobian_wrt_input_sandwich_diag_to_diag(feat_in, feat_out, diag_tmp)\n",
    "\n",
    "##################################################################################\n",
    "print(slow_diag_Jt_tmp_J.shape, lessslow_diag_Jt_tmp_J.shape, fast_diag_Jt_tmp_J.shape)\n",
    "assert torch.max(torch.abs(slow_diag_Jt_tmp_J - lessslow_diag_Jt_tmp_J)) < 1e-5\n",
    "assert torch.max(torch.abs(slow_diag_Jt_tmp_J - fast_diag_Jt_tmp_J)) < 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagonal approximation wrt weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 9]) torch.Size([2, 9]) torch.Size([2, 9])\n"
     ]
    }
   ],
   "source": [
    "diag_tmp = torch.diagonal(tmp, dim1=1, dim2=2)\n",
    "\n",
    "# SLOW METHOD\n",
    "# upscale tmp diagonal to full matrix\n",
    "tmp_simple = torch.diag_embed(diag_tmp)\n",
    "# compute J^T * tmp * J\n",
    "slow_Jt_tmp_J = torch.einsum('Bji,Bjk,Bkq->Biq', conv._jacobian_wrt_weight(feat_in, feat_out), tmp_simple, conv._jacobian_wrt_weight(feat_in, feat_out))\n",
    "# take only the diagonal (and discard all other elements)\n",
    "slow_diag_Jt_tmp_J = torch.diagonal(slow_Jt_tmp_J, dim1=1, dim2=2)\n",
    "\n",
    "# LESS SLOW METHOD\n",
    "# # compute J^T * tmp * J\n",
    "lessslow_Jt_tmp_J = conv._jacobian_wrt_weight_sandwich_full_to_full(feat_in, feat_out, tmp_simple)\n",
    "# take only the diagonal (and discard all other elements)\n",
    "lessslow_diag_Jt_tmp_J = torch.diagonal(lessslow_Jt_tmp_J, dim1=1, dim2=2)\n",
    "\n",
    "# FAST METHOD\n",
    "fast_diag_Jt_tmp_J  = conv._jacobian_wrt_weight_sandwich_diag_to_diag(feat_in, feat_out, diag_tmp)\n",
    "\n",
    "##################################################################################\n",
    "print(slow_diag_Jt_tmp_J.shape, lessslow_diag_Jt_tmp_J.shape, fast_diag_Jt_tmp_J.shape)\n",
    "assert torch.max(torch.abs(slow_diag_Jt_tmp_J - lessslow_diag_Jt_tmp_J)) < 1e-5\n",
    "assert torch.max(torch.abs(slow_diag_Jt_tmp_J - fast_diag_Jt_tmp_J)) < 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5e16edc4df7b202951e9e3c113e27071ab15724ce07cdee44fb794505e05de3d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
